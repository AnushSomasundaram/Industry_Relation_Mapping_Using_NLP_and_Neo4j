{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfc9201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------\n",
    "from neo4j import GraphDatabase\n",
    "import feedparser\n",
    "\n",
    "# newspaper needs lxml_html_clean for cleaning HTML\n",
    "try:\n",
    "    from newspaper import Article\n",
    "except ImportError:\n",
    "    raise ImportError(\n",
    "        \"Missing required module for newspaper HTML cleaning.\\n\"\n",
    "        \"Install with: pip install newspaper3k lxml_html_clean\"\n",
    "    )\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.schema import Document\n",
    "try:\n",
    "    from langchain_experimental.graphs.neo4j import Neo4jGraph, Neo4jGraphRetriever\n",
    "except ImportError:\n",
    "    raise ImportError(\n",
    "        \"Missing langchain_experimental package or its experimental graphs module.\\n\"\n",
    "        \"Install with: pip install langchain-experimental\"\n",
    "    )\n",
    "from langchain.embeddings import OllamaEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import Tool, initialize_agent, AgentType\n",
    "\n",
    "# Replace with your own credentials & endpoint\n",
    "NEO4J_URI = \"bolt://localhost:7687\"\n",
    "NEO4J_USER = \"neo4j\"\n",
    "NEO4J_PWD = \"your_password\"\n",
    "OLLAMA_EMBED_MODEL = \"ollama/text-embedding-ada-002\"\n",
    "\n",
    "# Create Neo4j driver\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PWD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600ae819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Step 3: Newsâ€Fetch Function\n",
    "# ---------------------------\n",
    "def fetch_latest_articles(industry: str, limit: int = 5) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Scrape Google News RSS for `industry` and return up to `limit` articles.\n",
    "    Each dict contains: title, url, snippet, published.\n",
    "    \"\"\"\n",
    "    feed_url = f\"https://news.google.com/rss/search?q={industry.replace(' ', '+')}\"\n",
    "    feed = feedparser.parse(feed_url)\n",
    "    results = []\n",
    "    for entry in feed.entries[:limit]:\n",
    "        art = Article(entry.link)\n",
    "        art.download(); art.parse()\n",
    "        results.append({\n",
    "            \"title\":     entry.title,\n",
    "            \"url\":       entry.link,\n",
    "            \"snippet\":   art.text[:300] + \"...\",\n",
    "            \"published\": entry.published,\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# %%\n",
    "# Step 4: Ingest Article into Neo4j\n",
    "# ----------------------------------\n",
    "def ingest_article(industry: str, info: dict):\n",
    "    \"\"\"\n",
    "    Merge the Industry node, create an Article node and link it.\n",
    "    \"\"\"\n",
    "    query = \"\"\"\n",
    "    MERGE (c:Class {name:$industry})\n",
    "    CREATE (a:Article {\n",
    "      title:$title, url:$url,\n",
    "      snippet:$snippet, published:$published\n",
    "    })\n",
    "    MERGE (a)-[:ABOUT]->(c)\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"industry\":  industry,\n",
    "        \"title\":     info[\"title\"],\n",
    "        \"url\":       info[\"url\"],\n",
    "        \"snippet\":   info[\"snippet\"],\n",
    "        \"published\": info[\"published\"],\n",
    "    }\n",
    "    with driver.session() as sess:\n",
    "        sess.run(query, params)\n",
    "\n",
    "# %%\n",
    "# Step 5: Combined Fetch & Ingest Tool\n",
    "# -------------------------------------\n",
    "def fetch_and_ingest(industry: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch latest articles and write them into Neo4j.\n",
    "    Returns a summary string.\n",
    "    \"\"\"\n",
    "    articles = fetch_latest_articles(industry)\n",
    "    for art in articles:\n",
    "        ingest_article(industry, art)\n",
    "    return f\"Ingested {len(articles)} articles for '{industry}' into Neo4j.\" \n",
    "\n",
    "# %%\n",
    "# Step 6: Load Documents from Neo4j for Vector Index\n",
    "# ---------------------------------------------------\n",
    "def load_docs_for_embeddings() -> list[Document]:\n",
    "    \"\"\"\n",
    "    Pull all Class node descriptions and Article texts as Documents.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    query = \"\"\"\n",
    "    MATCH (c:Class)\n",
    "      OPTIONAL MATCH (a:Article)-[:ABOUT]->(c)\n",
    "    RETURN c.name AS industry, c.description AS desc, collect(a.snippet) AS snippets\n",
    "    \"\"\"\n",
    "    with driver.session() as sess:\n",
    "        for rec in sess.run(query):\n",
    "            text = (rec[\"desc\"] or \"\") + \"\\n\\n\" + \"\\n---\\n\".join(rec[\"snippets\"])\n",
    "            docs.append(\n",
    "                Document(\n",
    "                    page_content=text,\n",
    "                    metadata={\"industry\": rec[\"industry\"]}\n",
    "                )\n",
    "            )\n",
    "    return docs\n",
    "\n",
    "# %%\n",
    "# Step 7: Build Vector Store\n",
    "# --------------------------\n",
    "all_docs = load_docs_for_embeddings()\n",
    "emb = OllamaEmbeddings(model=OLLAMA_EMBED_MODEL)\n",
    "vector_store = FAISS.from_documents(all_docs, emb)\n",
    "\n",
    "# (Optional) persist index\n",
    "# vector_store.save_local(\"industry_articles_faiss\")\n",
    "\n",
    "# %%\n",
    "# Step 8: Configure Retrievers\n",
    "# ----------------------------\n",
    "# Graph Retriever\n",
    "graph = Neo4jGraph(uri=NEO4J_URI, username=NEO4J_USER, password=NEO4J_PWD)\n",
    "graph_retriever = Neo4jGraphRetriever(\n",
    "    graph=graph,\n",
    "    node_label=\"Class\",\n",
    "    relationship_type=\"ABOUT\",  # or your custom edge type\n",
    "    traversal_depth=2,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "# Semantic Retriever\n",
    "semantic_retriever = vector_store.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# %%\n",
    "# Step 9: Wrap Tools & Initialize Agent\n",
    "# --------------------------------------\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"fetch_news\",\n",
    "        func=fetch_and_ingest,\n",
    "        description=\"Fetch and ingest latest news articles for an industry\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"graph_search\",\n",
    "        func=graph_retriever.get_relevant_nodes,\n",
    "        description=\"Traverse industry graph for related nodes\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"semantic_search\",\n",
    "        func=semantic_retriever.get_relevant_documents,\n",
    "        description=\"Find semantically relevant docs via FAISS\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "agent = initialize_agent(\n",
    "    tools, llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# %%\n",
    "# Step 10: Run Example Queries\n",
    "# -----------------------------\n",
    "# 1) Ingest fresh news for 'Automotive'\n",
    "print(agent.run(\"fetch_news Automotive\"))\n",
    "\n",
    "# 2) Ask a hybrid question\n",
    "resp = agent.run(\"What do you know about the 'Renewable Energy' industry and its top two-hop neighbors?\")\n",
    "print(resp)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
